{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN4VGDsh9oMskcf7XEx+BRr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rencdr/python.RL.CartPole/blob/master/RL_CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXNx8myafFo8"
      },
      "outputs": [],
      "source": [
        "!pip install gym\n",
        "\n",
        "import gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")"
      ],
      "metadata": {
        "id": "fFyFBDBJfMuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-değer tablosunu başlat\n",
        "# Gözlem alanını discretize etmek için örnek: [pozisyon, hız, açı, açı hızı]\n",
        "# Bu örnekte, her bir özellik için 10 aralık seçiyoruz\n",
        "state_space_bins = [10, 10, 10, 10]\n",
        "action_space_bins = [env.action_space.n]\n",
        "Q_table = np.random.uniform(low=-1, high=1, size=(state_space_bins + action_space_bins))"
      ],
      "metadata": {
        "id": "w1MQEQ9mfW3I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-learning parametreleri\n",
        "learning_rate = 0.1\n",
        "discount_factor = 0.99\n",
        "exploration_prob = 0.2"
      ],
      "metadata": {
        "id": "5geBpOIlfdOD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eğitim parametreleri\n",
        "num_episodes = 1000"
      ],
      "metadata": {
        "id": "hNMygf5offdy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eğitim Döngüsü\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    state_bins = [np.digitize(state[i], bins=np.linspace(-4.8, 4.8, state_space_bins[i]-1)) for i in range(len(state))]\n",
        "    total_reward = 0\n",
        "\n",
        "    while True:\n",
        "        if np.random.rand() < exploration_prob:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = int(np.argmax(Q_table[state_bins]))\n",
        "\n",
        "        action = np.clip(action, 0, env.action_space.n - 1)\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state_bins = [np.digitize(next_state[i], bins=np.linspace(-4.8, 4.8, state_space_bins[i]-1)) for i in range(len(next_state))]\n",
        "        Q_table[state_bins + [action]] += learning_rate * (reward + discount_factor * np.max(Q_table[next_state_bins]) - Q_table[state_bins + [action]])\n",
        "\n",
        "        state = next_state\n",
        "        state_bins = next_state_bins\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tnGtBf9jfhH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eğitim sonrasında ajanın performansını gözlemleme\n",
        "state = env.reset()\n",
        "state_bins = [np.digitize(state[i], bins=np.linspace(-4.8, 4.8, state_space_bins[i]-1)) for i in range(len(state))]\n",
        "total_reward = 0\n",
        "\n",
        "while True:\n",
        "    # Düzeltilmiş kısım\n",
        "    action = int(np.argmax(Q_table[state_bins]))  # Q-değerlerinden en büyük olan indeksi al\n",
        "    action = np.clip(action, 0, env.action_space.n - 1)  # Aksiyon uzayının sınırları dışına çıkmasını engelle\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    next_state_bins = [np.digitize(next_state[i], bins=np.linspace(-4.8, 4.8, state_space_bins[i]-1)) for i in range(len(next_state))]\n",
        "\n",
        "    state = next_state\n",
        "    state_bins = next_state_bins\n",
        "    total_reward += reward\n",
        "\n",
        "    env.render()\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(f\"Eğitilmiş Ajanın Toplam Ödülü: {total_reward}\")"
      ],
      "metadata": {
        "id": "FagqNLL-g-Ja"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}